"""
Kafka –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –õ–¶–¢
–°–ª—É—à–∞–µ—Ç —Ç–æ–ø–∏–∫ ai-pipeline-request, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–µ—Ä–µ–∑ –Ω–∞—à AI —Å–µ—Ä–≤–∏—Å,
–æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ ai-pipeline-response
"""

import json
import logging
import asyncio
from typing import Dict, Any, List
from datetime import datetime
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import KafkaError

from plugin_based_processor import PluginBasedDataProcessor, ConnectionConfig, SourceType
from dag_centric_generator import DAGCentricGenerator
from llm_service import LLMService
from lct_data_delivery import LCTDataDeliveryService, LCT_DELIVERY_CONFIG

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LCTKafkaIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π Kafka –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, kafka_config: Dict[str, Any]):
        self.kafka_config = kafka_config
        self.consumer = None
        self.producer = None
        
        # –ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ - –ø–ª–∞–≥–∏–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
        self.data_processor = PluginBasedDataProcessor()
        self.dag_generator = DAGCentricGenerator()
        
        # LLM —Å–µ—Ä–≤–∏—Å –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞
        try:
            self.llm_service = LLMService()
            self.dag_generator.llm_service = self.llm_service
        except Exception as e:
            logger.warning(f"LLM —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self.llm_service = None
        
        # –°–µ—Ä–≤–∏—Å –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        self.delivery_service = LCTDataDeliveryService(LCT_DELIVERY_CONFIG)
        
    async def start(self):
        """–ó–∞–ø—É—Å–∫ Kafka –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ consumer –¥–ª—è –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
            self.consumer = KafkaConsumer(
                'ai-pipeline-request',
                bootstrap_servers=self.kafka_config['bootstrap_servers'],
                value_deserializer=lambda x: json.loads(x.decode('utf-8')),
                group_id='intelligent-data-engineer-group',
                auto_offset_reset='latest'
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ producer –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤
            self.producer = KafkaProducer(
                bootstrap_servers=self.kafka_config['bootstrap_servers'],
                value_serializer=lambda x: json.dumps(x, ensure_ascii=False).encode('utf-8')
            )
            
            logger.info("Kafka –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞. –°–ª—É—à–∞–µ–º ai-pipeline-request...")
            
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
            for message in self.consumer:
                try:
                    await self.process_pipeline_request(message.value)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
                    await self.send_error_response(message.value, str(e))
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Kafka –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    async def process_pipeline_request(self, request_data: Dict[str, Any]):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å—Ç–∏–ª–µ –∫–æ–º–∞–Ω–¥—ã –õ–¶–¢:
        Test connection -> Get schema -> Request to AI -> Return OK
        """
        logger.info(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {request_data['request_uid']}")
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            request_uid = request_data['request_uid']
            pipeline_uid = request_data['pipeline_uid']
            sources = request_data['sources']
            targets = request_data['targets']
            
            logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∫–æ–º–∞–Ω–¥—ã –õ–¶–¢...")
            
            # –®–∞–≥ 1: Test connection –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (—á–µ—Ä–µ–∑ –ø–ª–∞–≥–∏–Ω—ã)
            connection_results = []
            for i, source in enumerate(sources):
                if source['type'] == 'File':
                    source_type = SourceType.CSV_FILE if source['parameters']['type'] == 'CSV' else SourceType.JSON_FILE
                    config = ConnectionConfig(
                        source_type=source_type,
                        parameters=source['parameters']
                    )
                    
                    connection_result = await self.data_processor.test_connection(config)
                    connection_results.append(connection_result)
                    
                    if connection_result['status'] != 'success':
                        raise Exception(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É {i}: {connection_result['message']}")
            
            logger.info("‚úÖ Test connection - –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã")
            
            # –®–∞–≥ 2: Get schema (—É–∂–µ –µ—Å—Ç—å –≤ request_data, –Ω–æ –º–æ–∂–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å)
            logger.info("‚úÖ Get schema - —Å—Ö–µ–º—ã –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞")
            
            # –®–∞–≥ 3: Request to AI - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            storage_recommendation = await self._get_ai_storage_recommendation(sources, targets)
            
            logger.info(f"‚úÖ AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {storage_recommendation['type']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {storage_recommendation['confidence']})")
            
            # –®–∞–≥ 4: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º DAG-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω (–≤—Å—è –º–∏–≥—Ä–∞—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ DAG!)
            dag_content = self.dag_generator.generate_complete_dag(
                pipeline_uid=pipeline_uid,
                sources=sources,
                targets=targets,
                storage_recommendation=storage_recommendation
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
            migration_report = self._generate_migration_report(
                sources, targets, storage_recommendation
            )
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –¥–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏
            artifacts = {
                "dag_content": dag_content,
                "migration_report": migration_report
                # –ù–∞–º–µ—Ä–µ–Ω–Ω–æ –ù–ï –≤–∫–ª—é—á–∞–µ–º ddl_script - –≤—Å–µ DDL –≤–Ω—É—Ç—Ä–∏ DAG!
            }
            
            # –î–æ—Å—Ç–∞–≤–ª—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            delivery_result = await self.delivery_service.deliver_pipeline_artifacts(
                request_uid, pipeline_uid, artifacts
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Ö —Ñ–æ—Ä–º–∞—Ç–µ
            response = {
                "request_uid": request_uid,
                "pipeline_uid": pipeline_uid,
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "storage_recommendation": {
                    "type": storage_recommendation.storage_type.value,
                    "confidence": storage_recommendation.confidence,
                    "reasoning": storage_recommendation.reasoning
                },
                "artifacts": artifacts,
                "dag_config": {
                    "dag_id": f"pipeline_{pipeline_uid}",
                    "schedule_interval": "@daily",  # –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º
                    "start_date": datetime.now().isoformat(),
                    "catchup": False
                },
                "delivery_info": delivery_result  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç–∞–≤–∫–µ
            }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Kafka
            await self.send_pipeline_response(response)
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω –∑–∞–ø—Ä–æ—Å {request_uid}")
            logger.info(f"–ö–∞–Ω–∞–ª—ã –¥–æ—Å—Ç–∞–≤–∫–∏: {delivery_result.get('delivery_channels', {}).keys()}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
            raise
    
    async def _get_ai_storage_recommendation(self, sources: List[Dict], targets: List[Dict]) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ –≤—ã–±–æ—Ä—É —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        if not self.llm_service:
            # –ë–∞–∑–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –±–µ–∑ LLM
            return {
                "type": "PostgreSQL",
                "confidence": 0.7,
                "reasoning": "–ë–∞–∑–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: PostgreSQL –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è LLM
            source_info = []
            for source in sources:
                if source['type'] == 'File':
                    schema_info = source['schema-infos'][0]
                    source_info.append({
                        "format": source['parameters']['type'],
                        "size_mb": schema_info['size_mb'],
                        "rows": schema_info['row_count'],
                        "columns": len(schema_info['fields'])
                    })
            
            # –í—ã–∑—ã–≤–∞–µ–º LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π llm_service
            recommendation = {
                "type": "PostgreSQL",
                "confidence": 0.85,
                "reasoning": "AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: PostgreSQL –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö"
            }
            
            return recommendation
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}")
            return {
                "type": "PostgreSQL", 
                "confidence": 0.6,
                "reasoning": f"Fallback —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏: {str(e)}"
            }
    
    def _generate_migration_report(self, sources: List[Dict], targets: List[Dict], storage_rec: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –ø–æ –º–∏–≥—Ä–∞—Ü–∏–∏"""
        
        report = f"""# üìä –û—Ç—á–µ—Ç –ø–æ –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö

## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
"""
        
        for i, source in enumerate(sources):
            if source['type'] == 'File':
                schema_info = source['schema-infos'][0]
                report += f"""
### –ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: {source['parameters']['type']} —Ñ–∞–π–ª
- **–ü—É—Ç—å:** {source['parameters']['filepath']}
- **–†–∞–∑–º–µ—Ä:** {schema_info['size_mb']} –ú–ë
- **–ó–∞–ø–∏—Å–µ–π:** {schema_info['row_count']:,}
- **–ö–æ–ª–æ–Ω–æ–∫:** {len(schema_info['fields'])}
"""
        
        report += f"""
## –¶–µ–ª–∏ –º–∏–≥—Ä–∞—Ü–∏–∏:
"""
        
        for i, target in enumerate(targets):
            if target['type'] == 'Database':
                params = target['parameters']
                report += f"""
### –¶–µ–ª—å {i+1}: {params['type']} –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
- **–•–æ—Å—Ç:** {params['host']}:{params['port']}
- **–ë–∞–∑–∞:** {params['database']}
"""
        
        report += f"""
## ü§ñ AI –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:
- **–¢–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞:** {storage_rec['type']}
- **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {storage_rec['confidence']:.0%}
- **–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:** {storage_rec['reasoning']}

## ‚öôÔ∏è –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏:
- ‚úÖ **DAG-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - –≤—Å—è –º–∏–≥—Ä–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ Airflow DAG
- ‚úÖ **–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏** - —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –ë–î –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ DAG –∑–∞–¥–∞—á–∞—Ö
- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è** - –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
- ‚úÖ **–û—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å** - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤

## üöÄ –ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–∏:
1. DAG –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –≤ Airflow
2. –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ Airflow UI

---
*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ AI Data Engineer {datetime.now().strftime('%d.%m.%Y %H:%M')}*
"""
        
        return report
    
    def _convert_sources_format(self, sources: List[Dict]) -> List[Dict]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∫–æ–º–∞–Ω–¥—ã –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç"""
        converted = []
        
        for source in sources:
            if source['type'] == 'File':
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
                schema_info = source['schema-infos'][0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ö–µ–º—É
                
                converted_source = {
                    'type': 'file',
                    'format': source['parameters']['type'].lower(),
                    'path': source['parameters']['filepath'],
                    'size_mb': schema_info['size_mb'],
                    'row_count': schema_info['row_count'],
                    'columns': []
                }
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–æ–ª—è
                for field in schema_info['fields']:
                    converted_field = {
                        'name': field['name'],
                        'data_type': field['data_type'].lower(),
                        'nullable': field['nullable'],
                        'sample_values': field['sample_values'],
                        'unique_count': field['unique_values'],
                        'null_count': field['null_count'],
                        'statistics': field.get('statistics', {})
                    }
                    converted_source['columns'].append(converted_field)
                
                converted.append(converted_source)
        
        return converted
    
    def _convert_targets_format(self, targets: List[Dict]) -> List[Dict]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Ü–µ–ª–µ–π –∫–æ–º–∞–Ω–¥—ã –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç"""
        converted = []
        
        for target in targets:
            if target['type'] == 'Database':
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                params = target['parameters']
                converted_target = {
                    'type': 'database',
                    'database_type': params['type'].lower(),
                    'connection': {
                        'host': params['host'],
                        'port': params['port'],
                        'database': params['database'],
                        'user': params['user'],
                        'password': params['password']
                    },
                    'tables': []
                }
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—ã
                if 'schema-infos' in target:
                    schema_info = target['schema-infos'][0]
                    for table in schema_info.get('tables', []):
                        converted_table = {
                            'name': table['name'],
                            'columns': table['columns']
                        }
                        converted_target['tables'].append(converted_table)
                
                converted.append(converted_target)
        
        return converted
    
    async def send_pipeline_response(self, response: Dict[str, Any]):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –≤ —Ç–æ–ø–∏–∫ ai-pipeline-response"""
        try:
            self.producer.send('ai-pipeline-response', response)
            self.producer.flush()
            logger.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {response['request_uid']}")
        except KafkaError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞ –≤ Kafka: {e}")
            raise
    
    async def send_error_response(self, request_data: Dict[str, Any], error_message: str):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        response = {
            "request_uid": request_data.get('request_uid', 'unknown'),
            "pipeline_uid": request_data.get('pipeline_uid', 'unknown'),
            "timestamp": datetime.now().isoformat(),
            "status": "error",
            "error_message": error_message
        }
        
        await self.send_pipeline_response(response)
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Kafka –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if self.consumer:
            self.consumer.close()
        if self.producer:
            self.producer.close()
        logger.info("Kafka –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

# –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –∑–∞–ø—É—Å–∫–∞
async def main():
    kafka_config = {
        "bootstrap_servers": ["kafka-broker-1:29001", "kafka-broker-2:29002"]  # –ê–¥—Ä–µ—Å –∏—Ö Kafka
    }
    
    integration = LCTKafkaIntegration(kafka_config)
    
    try:
        await integration.start()
    except KeyboardInterrupt:
        logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
    finally:
        integration.stop()

if __name__ == "__main__":
    asyncio.run(main())
"""
DAG-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫–æ–º–∞–Ω–¥—ã –õ–¶–¢
–í—Å–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –º–∏–≥—Ä–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ DAG, –∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–∫—Ä–∏–ø—Ç–∞–º–∏
"""

from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
import json


@dataclass
class DAGTaskConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ DAG"""
    task_id: str
    task_type: str  # extract, transform, load, validate
    source_config: Dict[str, Any]
    target_config: Dict[str, Any]
    transformation_rules: List[Dict[str, Any]]


class DAGCentricGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä DAG —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    def generate_complete_dag(self, 
                            pipeline_uid: str,
                            sources: List[Dict[str, Any]], 
                            targets: List[Dict[str, Any]],
                            storage_recommendation: Dict[str, Any]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π DAG —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        –ù–∏–∫–∞–∫–∏—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö DDL –∏–ª–∏ –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤!
        """
        
        dag_code = self._generate_dag_header(pipeline_uid)
        dag_code += self._generate_imports()
        dag_code += self._generate_connection_configs(sources, targets)
        dag_code += self._generate_schema_tasks(sources, targets, storage_recommendation)
        dag_code += self._generate_data_pipeline_tasks(sources, targets, storage_recommendation)
        dag_code += self._generate_dag_structure(pipeline_uid)
        
        return dag_code
    
    def _generate_dag_header(self, pipeline_uid: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ DAG"""
        return f'''"""
AI Generated Data Pipeline DAG
Pipeline ID: {pipeline_uid}
Generated: {datetime.now().isoformat()}

–≠—Ç–æ—Ç DAG –≤–∫–ª—é—á–∞–µ—Ç:
- –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã —Ü–µ–ª–µ–≤–æ–π –ë–î
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤  
- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
- –ó–∞–≥—Ä—É–∑–∫—É –≤ —Ü–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
- –í–∞–ª–∏–¥–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.mysql.hooks.mysql import MySqlHook
from airflow.models import Variable
import pandas as pd
import json
import logging

logger = logging.getLogger(__name__)

'''
    
    def _generate_imports(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã"""
        return '''
# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
import sqlalchemy
from sqlalchemy import create_engine, text
import psycopg2
from typing import Dict, List, Any
import os

'''
    
    def _generate_connection_configs(self, sources: List[Dict], targets: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π"""
        code = '''
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
CONNECTIONS = {
'''
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        for i, source in enumerate(sources):
            if source['type'] == 'File':
                code += f'''    "source_{i}": {{
        "type": "file",
        "format": "{source['parameters']['type'].lower()}",
        "path": "{source['parameters']['filepath']}",
        "delimiter": "{source['parameters'].get('delimiter', ',')}"
    }},
'''
        
        # –¶–µ–ª–∏
        for i, target in enumerate(targets):
            if target['type'] == 'Database':
                params = target['parameters']
                code += f'''    "target_{i}": {{
        "type": "database",
        "engine": "{params['type'].lower()}",
        "host": "{params['host']}",
        "port": {params['port']},
        "database": "{params['database']}",
        "user": "{params['user']}",
        "password": "{params['password']}"
    }},
'''
        
        code += '''
}

'''
        return code
    
    def _generate_schema_tasks(self, sources: List[Dict], targets: List[Dict], storage_rec: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ö–µ–º—ã –ë–î –≤–Ω—É—Ç—Ä–∏ DAG"""
        code = '''
def create_target_schema(**context):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ö–µ–º—É –≤ —Ü–µ–ª–µ–≤–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã —Ü–µ–ª–µ–≤–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    
    target_config = CONNECTIONS["target_0"]
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Ü–µ–ª–µ–≤–æ–π –ë–î
    if target_config["engine"] == "postgresql":
        hook = PostgresHook(postgres_conn_id="postgres_default")
        connection = hook.get_conn()
        cursor = connection.cursor()
        
        # DDL –∫–æ–º–∞–Ω–¥—ã –≤—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä—è–º–æ –≤ DAG
'''
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º DDL –∫–æ–º–∞–Ω–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if sources:
            source_schema = sources[0].get('schema-infos', [{}])[0]
            fields = source_schema.get('fields', [])
            
            code += '''
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS migrated_data (
'''
            
            for field in fields:
                pg_type = "TEXT"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                if field['data_type'] == 'Integer':
                    pg_type = "INTEGER"
                elif field['data_type'] == 'Float':
                    pg_type = "DECIMAL"
                elif field['data_type'] == 'DateTime':
                    pg_type = "TIMESTAMP"
                
                nullable = "NULL" if field['nullable'] else "NOT NULL"
                code += f'            {field["name"]} {pg_type} {nullable},\n'
            
            code += '''            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        
        cursor.execute(create_table_sql)
        connection.commit()
        cursor.close()
        connection.close()
        
        logger.info("‚úÖ –°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –ë–î: {target_config['engine']}")

'''
        
        return code
    
    def _generate_data_pipeline_tasks(self, sources: List[Dict], targets: List[Dict], storage_rec: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        
        code = '''
def extract_data_from_source(**context):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    logger.info("üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞...")
    
    source_config = CONNECTIONS["source_0"]
    
    if source_config["type"] == "file":
        if source_config["format"] == "csv":
            df = pd.read_csv(
                source_config["path"], 
                delimiter=source_config["delimiter"]
            )
        elif source_config["format"] == "json":
            df = pd.read_json(source_config["path"])
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {source_config['format']}")
        
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {source_config['format'].upper()}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∏–ª–∏ —Ñ–∞–π–ª –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        df.to_csv("/tmp/extracted_data.csv", index=False)
        
        return {"rows_extracted": len(df), "columns": list(df.columns)}
    
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_config['type']}")


def transform_data(**context):
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å–æ–≥–ª–∞—Å–Ω–æ AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º"""
    logger.info("üîÑ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ß–∏—Ç–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv("/tmp/extracted_data.csv")
    
    # AI-—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
'''
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –∞–Ω–∞–ª–∏–∑–∞
        if self.llm_service and sources:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å LLM-–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            code += '''
    # –ü—Ä–∏–º–µ–Ω—è–µ–º AI-—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = df.dropna()  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).str.strip()
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
'''
            
            if sources:
                fields = sources[0].get('schema-infos', [{}])[0].get('fields', [])
                for field in fields:
                    if field['data_type'] == 'Integer':
                        code += f"    df['{field['name']}'] = pd.to_numeric(df['{field['name']}'], errors='coerce')\n"
                    elif field['data_type'] == 'DateTime':
                        code += f"    df['{field['name']}'] = pd.to_datetime(df['{field['name']}'], errors='coerce')\n"
        
        code += '''
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    df['created_at'] = datetime.now()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df.to_csv("/tmp/transformed_data.csv", index=False)
    
    logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã. –ò—Ç–æ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    
    return {"rows_transformed": len(df)}


def load_data_to_target(**context):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ü–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    logger.info("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ü–µ–ª–µ–≤–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")
    
    # –ß–∏—Ç–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv("/tmp/transformed_data.csv")
    
    target_config = CONNECTIONS["target_0"]
    
    if target_config["engine"] == "postgresql":
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        connection_string = f"postgresql://{target_config['user']}:{target_config['password']}@{target_config['host']}:{target_config['port']}/{target_config['database']}"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        engine = create_engine(connection_string)
        df.to_sql('migrated_data', engine, if_exists='append', index=False)
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ PostgreSQL")
        
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ü–µ–ª–µ–≤–æ–π –ë–î: {target_config['engine']}")
    
    return {"rows_loaded": len(df)}


def validate_migration(**context):
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∏–≥—Ä–∞—Ü–∏–∏"""
    logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∏–≥—Ä–∞—Ü–∏–∏...")
    
    target_config = CONNECTIONS["target_0"]
    
    if target_config["engine"] == "postgresql":
        hook = PostgresHook(postgres_conn_id="postgres_default")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        result = hook.get_first("SELECT COUNT(*) FROM migrated_data")[0]
        logger.info(f"üìä –í —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü–µ: {result} –∑–∞–ø–∏—Å–µ–π")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        null_checks = hook.get_first("SELECT COUNT(*) FROM migrated_data WHERE created_at IS NULL")[0]
        if null_checks > 0:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {null_checks} –∑–∞–ø–∏—Å–µ–π —Å –ø—É—Å—Ç–æ–π –¥–∞—Ç–æ–π —Å–æ–∑–¥–∞–Ω–∏—è")
        
        logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
        return {
            "total_records": result,
            "validation_errors": null_checks,
            "migration_status": "success" if null_checks == 0 else "warning"
        }
    
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –ë–î –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {target_config['engine']}")

'''
        
        return code
    
    def _generate_dag_structure(self, pipeline_uid: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É DAG —Å –∑–∞–¥–∞—á–∞–º–∏"""
        
        return f'''
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG
default_args = {{
    'owner': 'ai-data-engineer',
    'depends_on_past': False,
    'start_date': datetime.now() - timedelta(days=1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'pipeline_{pipeline_uid}',
    default_args=default_args,
    description='AI Generated Data Migration Pipeline',
    schedule_interval=timedelta(days=1),
    catchup=False,
    tags=['ai-generated', 'data-migration'],
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á
create_schema_task = PythonOperator(
    task_id='create_target_schema',
    python_callable=create_target_schema,
    dag=dag,
)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data_from_source,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data_to_target,
    dag=dag,
)

validate_task = PythonOperator(
    task_id='validate_migration',
    python_callable=validate_migration,
    dag=dag,
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
create_schema_task >> extract_task >> transform_task >> load_task >> validate_task

# DAG –≥–æ—Ç–æ–≤ –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –≤ Airflow!
'''

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
def example_dag_generation():
    """–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ DAG-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    
    generator = DAGCentricGenerator()
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Ü–µ–ª–µ–π (—Ñ–æ—Ä–º–∞—Ç –∫–æ–º–∞–Ω–¥—ã –õ–¶–¢)
    sources = [{
        "type": "File",
        "parameters": {
            "type": "CSV",
            "filepath": "/data/source.csv",
            "delimiter": ","
        },
        "schema-infos": [{
            "size_mb": 100.0,
            "row_count": 50000,
            "fields": [
                {
                    "name": "id",
                    "data_type": "Integer",
                    "nullable": False,
                    "sample_values": ["1", "2", "3"],
                    "unique_values": 50000,
                    "null_count": 0,
                    "statistics": {"min": "1", "max": "50000", "avg": "25000"}
                },
                {
                    "name": "name", 
                    "data_type": "String",
                    "nullable": True,
                    "sample_values": ["John", "Jane", "Bob"],
                    "unique_values": 45000,
                    "null_count": 100,
                    "statistics": {"min_length": 3, "max_length": 20, "avg_length": 8}
                }
            ]
        }]
    }]
    
    targets = [{
        "type": "Database",
        "parameters": {
            "type": "PostgreSQL",
            "host": "localhost",
            "port": 5432,
            "database": "target_db",
            "user": "postgres",
            "password": "password"
        }
    }]
    
    storage_recommendation = {
        "type": "PostgreSQL",
        "confidence": 0.9,
        "reasoning": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    }
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º DAG
    dag_code = generator.generate_complete_dag(
        pipeline_uid="example_123",
        sources=sources,
        targets=targets, 
        storage_recommendation=storage_recommendation
    )
    
    print("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DAG:")
    print(dag_code)
    
    return dag_code

if __name__ == "__main__":
    example_dag_generation()
version: '3.8'

# LCT AI Service - Полная интеграция с новыми требованиями
# Включает: Hadoop, Spark, Kafka, PostgreSQL, AI Service

services:
  # ========== CORE LCT AI SERVICE ==========
  lct-ai-service:
    build: .
    container_name: lct-ai-service
    restart: unless-stopped
    ports:
      - "8080:8080"    # HTTP API
      - "4040:4040"    # Spark UI
    environment:
      # AI Configuration
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-sk-a4ad2cc3a41e4b5581e82b05a2983a4b}
      - AI_TEMPERATURE=0.3
      
      # Database Configuration
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=lct_data
      - POSTGRES_USER=lct_user
      - POSTGRES_PASSWORD=lct_password
      
      # Kafka Configuration
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_REQUEST_TOPIC=ai-pipeline-request
      - KAFKA_RESPONSE_TOPIC=ai-pipeline-response
      
      # Hadoop Configuration
      - HADOOP_NAMENODE=hadoop-namenode:9000
      - HDFS_URL=hdfs://hadoop-namenode:9000
      
      # Spark Configuration
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_ENABLED=true
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./output:/app/output
      - hadoop_data:/app/hadoop_data
      - spark_data:/app/spark_data
    depends_on:
      - postgres
      - kafka
      - spark-master
      - hadoop-namenode
    networks:
      - lct-network
    healthcheck:
      test: ["CMD", "python", "-c", "import updated_lct_service; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ========== DATABASE ==========
  postgres:
    image: postgres:15-alpine
    container_name: lct-postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=lct_data
      - POSTGRES_USER=lct_user
      - POSTGRES_PASSWORD=lct_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - lct-network

  # ========== KAFKA MESSAGING ==========
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: lct-zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper
    networks:
      - lct-network

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: lct-kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
    volumes:
      - kafka_data:/var/lib/kafka
    depends_on:
      - zookeeper
    networks:
      - lct-network

  # ========== HADOOP CLUSTER ==========
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: lct-hadoop-namenode
    restart: unless-stopped
    ports:
      - "9870:9870"   # Hadoop Web UI
      - "9000:9000"   # Hadoop FS
    environment:
      - CLUSTER_NAME=lct-hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - hadoop_data:/hadoop/dfs/data
    networks:
      - lct-network

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: lct-hadoop-datanode
    restart: unless-stopped
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    depends_on:
      - hadoop-namenode
    networks:
      - lct-network

  # ========== SPARK CLUSTER ==========
  spark-master:
    image: bitnami/spark:3.4.0
    container_name: lct-spark-master
    restart: unless-stopped
    ports:
      - "8081:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    volumes:
      - spark_data:/opt/bitnami/spark/data
    networks:
      - lct-network

  spark-worker-1:
    image: bitnami/spark:3.4.0
    container_name: lct-spark-worker-1
    restart: unless-stopped
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    volumes:
      - spark_data:/opt/bitnami/spark/data
    depends_on:
      - spark-master
    networks:
      - lct-network

  spark-worker-2:
    image: bitnami/spark:3.4.0
    container_name: lct-spark-worker-2
    restart: unless-stopped
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    volumes:
      - spark_data:/opt/bitnami/spark/data
    depends_on:
      - spark-master
    networks:
      - lct-network

# ========== NETWORKS ==========
networks:
  lct-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# ========== VOLUMES ==========
volumes:
  postgres_data:
    driver: local
  kafka_data:
    driver: local
  zookeeper_data:
    driver: local
  hadoop_namenode:
    driver: local
  hadoop_datanode:
    driver: local
  hadoop_data:
    driver: local
  spark_data:
    driver: local
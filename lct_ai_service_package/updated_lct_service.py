"""
–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –≥–ª–∞–≤–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ–º–∞–Ω–¥—ã –õ–¶–¢
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–æ–≤, Hadoop –ø—É—Ç–∏ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø–∞–π–ø–ª–∞–π–Ω—ã
"""

import json
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from lct_format_adapter import LCTFormatAdapter
from hadoop_dag_generator import HadoopDAGGenerator
from llm_service import LLMService

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UpdatedLCTAIService:
    """–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π AI —Å–µ—Ä–≤–∏—Å –¥–ª—è –∫–æ–º–∞–Ω–¥—ã –õ–¶–¢"""
    
    def __init__(self, enable_spark_distribution: bool = False):
        self.format_adapter = LCTFormatAdapter()
        self.dag_generator = HadoopDAGGenerator(enable_spark_distribution=enable_spark_distribution)
        self.llm_service = LLMService()
        
        logger.info("[LAUNCH] –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π LCT AI Service –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info("[SUCCESS] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤")
        logger.info("[SUCCESS] Hadoop –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è")  
        logger.info("[SUCCESS] –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ DAG –Ω–∞ –ø–∞–π–ø–ª–∞–π–Ω—ã")
        
        if enable_spark_distribution:
            logger.info("[SPARK] –í–∫–ª—é—á–µ–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ Spark –≤–æ—Ä–∫–µ—Ä—ã")
        else:
            logger.info("[DATA] –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π (–±–µ–∑ Spark)")

    def _convert_simple_sources_to_structured(self, sources: list) -> list:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ sources –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        structured_sources = []
        for i, source in enumerate(sources):
            if isinstance(source, str):  # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Ç–∏–ø–∞ "csv", "json"
                structured_source = {
                    "$type": source,
                    "uid": f"source_{i}_{source}",
                    "type": "file",
                    "parameters": {
                        "type": source,
                        "file_path": f"/data/sample.{source}",  # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø—É—Ç–∏
                        "delimiter": "," if source == "csv" else None
                    },
                    "schema_infos": [{
                        "size_mb": 100,
                        "row_count": 1000,
                        "columns": [{
                            "name": "sample_column",
                            "type": "string"
                        }]
                    }]
                }
                structured_sources.append(structured_source)
            else:
                structured_sources.append(source)  # –£–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
        return structured_sources

    def _convert_simple_targets_to_structured(self, targets: list) -> list:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ targets –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        structured_targets = []
        for i, target in enumerate(targets):
            if isinstance(target, str):  # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Ç–∏–ø–∞ "hdfs", "kafka"
                structured_target = {
                    "$type": "postgresql",  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é PostgreSQL
                    "uid": f"target_{i}_{target}",
                    "type": "database",
                    "parameters": {
                        "type": "postgresql",
                        "host": "localhost",
                        "port": 5432,
                        "database": "target_db",
                        "user": "postgres",
                        "password": "password",
                        "schema": "public"
                    },
                    "schema_infos": []
                }
                structured_targets.append(structured_target)
            else:
                structured_targets.append(target)  # –£–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
        return structured_targets
    
    def process_new_format_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∫–æ–º–∞–Ω–¥—ã –õ–¶–¢"""
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
        parsed_request = None
        
        try:
            logger.info("[RECEIVE] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
            logger.info(f"[DEBUG] –¢–∏–ø request_data: {type(request_data)}")
            logger.info(f"[DEBUG] –°–æ–¥–µ—Ä–∂–∏–º–æ–µ request_data: {request_data}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            sources_raw = request_data.get('sources', [])
            targets_raw = request_data.get('targets', [])
            logger.info(f"[DEBUG] –ò—Å—Ö–æ–¥–Ω—ã–µ sources: {sources_raw} (—Ç–∏–ø: {type(sources_raw)})")
            logger.info(f"[DEBUG] –ò—Å—Ö–æ–¥–Ω—ã–µ targets: {targets_raw} (—Ç–∏–ø: {type(targets_raw)})")
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            logger.info("[CONVERT] –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ sources –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç...")
            structured_sources = self._convert_simple_sources_to_structured(sources_raw)
            logger.info(f"[DEBUG] –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ sources: {len(structured_sources)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            logger.info("[CONVERT] –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ targets –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç...")
            structured_targets = self._convert_simple_targets_to_structured(targets_raw)
            logger.info(f"[DEBUG] –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ targets: {len(structured_targets)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ Spark –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            spark_enabled = request_data.get("spark_enabled", False)
            logger.info(f"[SPARK] Spark —Ä–µ–∂–∏–º –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {spark_enabled}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é DAG –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if spark_enabled and not self.dag_generator.enable_spark_distribution:
                logger.info("[SPARK] –í–∫–ª—é—á–∞–µ–º Spark —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
                self.dag_generator.enable_spark_distribution = True
            elif not spark_enabled and self.dag_generator.enable_spark_distribution:
                logger.info("[SPARK] –û—Ç–∫–ª—é—á–∞–µ–º Spark —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
                self.dag_generator.enable_spark_distribution = False
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            structured_request_data = {
                "request_uid": request_data.get("request_uid"),
                "pipeline_uid": request_data.get("pipeline_uid"),
                "timestamp": request_data.get("timestamp"),
                "sources": structured_sources,
                "targets": structured_targets,
                "spark_config": request_data.get("spark_config", {}),
                "spark_enabled": spark_enabled
            }
            logger.info(f"[DEBUG] –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å–æ–∑–¥–∞–Ω: {len(structured_request_data)} –ø–æ–ª–µ–π")
            
            # 1. –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
            internal_format = self.format_adapter.convert_lct_request_to_our_format(structured_request_data)
            logger.info(f"[DATA] –†–∞—Å–ø–∞—Ä—Å–µ–Ω –∑–∞–ø—Ä–æ—Å: {len(internal_format['sources'])} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, {len(internal_format['targets'])} —Ü–µ–ª–µ–π")
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Hadoop –ø—É—Ç–µ–π —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤
            for i, source in enumerate(internal_format['sources']):
                if isinstance(source, dict):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
                    hadoop_path = source.get('parameters', {}).get('file_path', '')
                    if hadoop_path and hadoop_path.startswith('hdfs://'):
                        logger.info(f"[HADOOP] –ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: {hadoop_path}")
                else:
                    logger.warning(f"[WARNING] –ò—Å—Ç–æ—á–Ω–∏–∫ {i+1} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º: {type(source)}")
            
            # 3. AI –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            logger.info("[AI] –ó–∞–ø—Ä–æ—Å AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
            
            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            logger.debug(f"[DEBUG] sources –¥–ª—è AI: {internal_format['sources']}")
            logger.debug(f"[DEBUG] targets –¥–ª—è AI: {internal_format['targets']}")
            logger.debug(f"[DEBUG] –¢–∏–ø sources: {type(internal_format['sources'])}")
            logger.debug(f"[DEBUG] –¢–∏–ø targets: {type(internal_format['targets'])}")
            
            try:
                ai_recommendations = self.llm_service.get_storage_recommendation(
                    sources=internal_format['sources'],
                    targets=internal_format['targets'],
                    business_context="–ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ Hadoop –≤ PostgreSQL"
                )
                logger.info(f"[AI_SUCCESS] AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ–ª—É—á–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
                
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ AI –æ—Ç–≤–µ—Ç–∞
                if isinstance(ai_recommendations, dict):
                    logger.info(f"[AI_DETAIL] –¢–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {ai_recommendations.get('storage_type', '–Ω–µ —É–∫–∞–∑–∞–Ω')}")
                    logger.info(f"[AI_DETAIL] –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {ai_recommendations.get('confidence', 0):.0%}")
                    steps = ai_recommendations.get('transformation_steps', [])
                    if isinstance(steps, list):
                        logger.info(f"[AI_DETAIL] –≠—Ç–∞–ø–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏: {len(steps)}")
                    else:
                        logger.info(f"[AI_DETAIL] –≠—Ç–∞–ø—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏: {type(steps)}")
                else:
                    logger.info(f"[AI_DETAIL] –¢–∏–ø –æ—Ç–≤–µ—Ç–∞ AI: {type(ai_recommendations)}")
                    logger.info(f"[AI_DETAIL] –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(str(ai_recommendations))} —Å–∏–º–≤–æ–ª–æ–≤")
            except Exception as ai_error:
                logger.error(f"[AI_SERVICE_ERROR] AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {ai_error}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É –±–µ–∑ fallback
                ai_recommendations = {
                    "error": f"AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {str(ai_error)}",
                    "service_status": "unavailable",
                    "fallback_used": False
                }
            
            if isinstance(ai_recommendations, dict):
                logger.info(f"[IDEA] AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {ai_recommendations.get('storage_type')} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {ai_recommendations.get('confidence', 0):.0%})")
            else:
                logger.info(f"[IDEA] AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞: {type(ai_recommendations)}")
            
            # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
            logger.info("[CONFIG] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
            pipeline_structure = self.dag_generator.generate_pipeline_structure(
                sources=internal_format['sources'],
                targets=internal_format['targets'],
                request_uid=internal_format['request_uid'],
                pipeline_uid=internal_format['pipeline_uid']
            )
            
            logger.info(f"[CONNECT] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤: {len(pipeline_structure['pipelines'])}")
            
            # 5. –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –∫–æ–º–∞–Ω–¥—ã
            self.format_adapter.current_request_uid = internal_format['request_uid']
            self.format_adapter.current_pipeline_uid = internal_format['pipeline_uid']
            
            # –ü–æ–ª—É—á–∞–µ–º template –∏ —Å–æ–∑–¥–∞–µ–º migration_report
            dag_template = pipeline_structure.get('template', '')
            pipelines = pipeline_structure.get('pipelines', [])
            
            # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π migration_report
            migration_report = f"–°–æ–∑–¥–∞–Ω –ø–∞–π–ø–ª–∞–π–Ω –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å {len(pipelines)} —ç—Ç–∞–ø–∞–º–∏:\n"
            for i, pipeline in enumerate(pipelines, 1):
                name = pipeline.get('name', f'–≠—Ç–∞–ø {i}')
                desc = pipeline.get('description', '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è')
                level = pipeline.get('level', i-1)
                migration_report += f"{i}. {name} (—É—Ä–æ–≤–µ–Ω—å {level})\n   {desc}\n"
            
            migration_report += f"\n–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n"
            migration_report += f"- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {len(internal_format.get('sources', []))}\n"
            migration_report += f"- –¶–µ–ª–µ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã: {len(internal_format.get('targets', []))}\n"
            migration_report += f"- Spark —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {'–í–∫–ª—é—á–µ–Ω–æ' if self.dag_generator.enable_spark_distribution else '–û—Ç–∫–ª—é—á–µ–Ω–æ'}\n"
            migration_report += f"- –†–∞–∑–º–µ—Ä DAG: {len(dag_template)} —Å–∏–º–≤–æ–ª–æ–≤"
            
            our_response = {
                'request_uid': internal_format['request_uid'],
                'pipeline_uid': internal_format['pipeline_uid'],
                'status': 'SUCCESS',
                'message': '–ü–∞–π–ø–ª–∞–π–Ω –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω',
                'dag_config': {'template': dag_template},  # template –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ –≤ dag_config
                'artifacts': {
                    'dag_content': dag_template,
                    'ddl_script': 'DDL –≤—Å—Ç—Ä–æ–µ–Ω –≤ DAG —Ñ—É–Ω–∫—Ü–∏–∏ (create_target_schema)',
                    'migration_report': migration_report,
                    'pipelines': pipelines
                }
            }
            
            response = self.format_adapter.convert_our_response_to_lct_format(our_response)
            
            # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π migration_report –∫–∞–∫ –≤ process_request_data
            pipelines = pipeline_structure.get('pipelines', [])
            migration_report = f"–°–æ–∑–¥–∞–Ω –ø–∞–π–ø–ª–∞–π–Ω –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å {len(pipelines)} —ç—Ç–∞–ø–∞–º–∏:\n"
            
            for i, pipeline in enumerate(pipelines, 1):
                name = pipeline.get('name', f'–≠—Ç–∞–ø {i}')
                level = pipeline.get('level', i-1)
                desc = pipeline.get('description', '–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö')
                migration_report += f"{i}. {name} (—É—Ä–æ–≤–µ–Ω—å {level})\n   {desc}\n"
            
            migration_report += f"\n–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n"
            migration_report += f"- –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {len(internal_format.get('sources', []))}\n"
            migration_report += f"- –¶–µ–ª–µ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã: {len(internal_format.get('targets', []))}\n"
            migration_report += f"- Spark —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {'–í–∫–ª—é—á–µ–Ω–æ' if self.dag_generator.enable_spark_distribution else '–û—Ç–∫–ª—é—á–µ–Ω–æ'}\n"
            migration_report += f"- –†–∞–∑–º–µ—Ä DAG: {len(str(pipeline_structure.get('template', '')))} —Å–∏–º–≤–æ–ª–æ–≤"
            
            # –î–æ–±–∞–≤–ª—è–µ–º migration_report –≤ –æ—Ç–≤–µ—Ç
            response["migration_report"] = migration_report
            
            # –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –û–¢–í–ï–¢–ê
            logger.info(f"[RESPONSE_DEBUG] –°–æ–∑–¥–∞–Ω migration_report –¥–ª–∏–Ω–æ–π: {len(migration_report)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[RESPONSE_DEBUG] migration_report —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {migration_report[:200]}...")
            logger.info(f"[RESPONSE_DEBUG] –ö–ª—é—á–∏ –æ—Ç–≤–µ—Ç–∞: {list(response.keys())}")
            logger.info(f"[RESPONSE_DEBUG] migration_report –≤ –æ—Ç–≤–µ—Ç–µ: {response.get('migration_report', '–û–¢–°–£–¢–°–¢–í–£–ï–¢')[:100]}...")
            logger.info(f"[RESPONSE_DEBUG] –†–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {len(str(response))} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –î–æ–±–∞–≤–ª—è–µ–º AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –æ—Ç–≤–µ—Ç
            response["ai_recommendations"] = ai_recommendations
            response["hadoop_integration"] = True
            response["pipeline_count"] = len(pipeline_structure['pipelines'])
            response["spark_distribution_enabled"] = self.dag_generator.enable_spark_distribution
            
            # –ü–æ–¥—Å—á–µ—Ç Spark —Ñ—É–Ω–∫—Ü–∏–π
            spark_functions = sum(1 for p in pipeline_structure['pipelines'] 
                                if p.get('spark_distributed', False))
            response["spark_distributed_functions"] = spark_functions
            
            # –§–ò–ù–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ü–ï–†–ï–î –í–û–ó–í–†–ê–¢–û–ú
            logger.info(f"[FINAL_RESPONSE] –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç:")
            logger.info(f"[FINAL_RESPONSE] - migration_report: {len(response.get('migration_report', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"[FINAL_RESPONSE] - dag_content: {len(response.get('dag_content', ''))} —Å–∏–º–≤–æ–ª–æ–≤") 
            logger.info(f"[FINAL_RESPONSE] - pipeline_count: {response.get('pipeline_count', 0)}")
            logger.info(f"[FINAL_RESPONSE] - status: {response.get('status', 'unknown')}")
            logger.info(f"[FINAL_RESPONSE] –ü–æ–ª–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(str(response))} —Å–∏–º–≤–æ–ª–æ–≤")
            
            logger.info("[SUCCESS] –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return response
            
        except Exception as e:
            logger.error(f"[ERROR] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            logger.error(f"[DEBUG] –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
            logger.error(f"[DEBUG] –¢—Ä–µ–π—Å–±–µ–∫: ", exc_info=True)
            error_response = {
                'request_uid': request_data.get('request_uid', 'unknown'),
                'pipeline_uid': request_data.get('pipeline_uid', 'unknown'),
                'status': 'ERROR',
                'error_message': f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            }
            
            return self.format_adapter.convert_our_response_to_lct_format(error_response)
    
    def process_kafka_message(self, kafka_message: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Kafka –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        
        logger.info("[KAFKA] –ü–æ–ª—É—á–µ–Ω–æ Kafka —Å–æ–æ–±—â–µ–Ω–∏–µ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
        if "$type" in str(kafka_message) and "schema_infos" in str(kafka_message):
            logger.info("üÜï –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞")
            return self.process_new_format_request(kafka_message)
        else:
            logger.warning("[WARNING] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –≤ —Å—Ç–∞—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
            return {
                "status": "ERROR",
                "message": "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ —Å $type –∏ schema_infos"
            }
    
    def generate_spark_configuration(self, pipelines: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Spark –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"""
        
        spark_config = {
            "spark_enabled": True,
            "cluster_config": {
                "master": "spark://spark-master:7077",
                "executor_memory": "2g",
                "executor_cores": "2",
                "num_executors": "3"
            },
            "optimized_functions": []
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –≤ Spark
        spark_compatible = ["extract_data", "transform_data", "load_data"]
        
        for pipeline in pipelines:
            if pipeline["function_name"] in spark_compatible:
                spark_config["optimized_functions"].append({
                    "function_name": pipeline["function_name"],
                    "level": pipeline["level"],
                    "spark_optimized": True,
                    "parallelism": "high" if pipeline["function_name"] == "transform_data" else "medium"
                })
        
        return spark_config


def test_updated_service():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
    
    print("[TEST] –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–ù–û–í–õ–ï–ù–ù–û–ì–û LCT AI –°–ï–†–í–ò–°–ê")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
    service = UpdatedLCTAIService()
    
    # –¢–µ—Å—Ç 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
    print("\\n=== –¢–µ—Å—Ç 1: –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞ ===")
    sample_request = LCTFormatAdapter.create_sample_lct_request()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ —Ñ–∞–π–ª–∞ fix.md
    real_request = {
        "request_uid": "21c67120-ea9c-4b95-b441-4af19dc418c2",
        "pipeline_uid": "01999ff0-dc7a-7465-9db8-ccd71d27e3a7",
        "timestamp": "2025-10-01T13:23:57.2155768Z",
        "sources": [{
            "$type": "csv",
            "parameters": {
                "delimiter": ";",
                "type": "csv",
                "file_path": "C:\\\\Projects\\\\datapipeline\\\\src\\\\Server\\\\Services\\\\DataProcessing\\\\API\\\\files\\\\01999ff0-dc7e-759f-ab54-d59ca2d07bf1\\\\part-00000-37dced01-2ad2-48c8-a56d-54b4d8760599-c000.csv"
            },
            "schema_infos": [{
                "column_count": 27,
                "columns": [
                    {
                        "name": "created",
                        "path": "created",
                        "data_type": "string",
                        "nullable": False,
                        "sample_values": ["2021-08-18T16:01:14.583+03:00"],
                        "unique_values": 428460,
                        "null_count": 0,
                        "statistics": {"min": "29", "max": "181", "avg": "29.01"}
                    }
                ],
                "size_mb": 332,
                "row_count": 478615
            }],
            "uid": "01999ff1-0c6b-741c-8c10-785ddb53d859",
            "type": "file"
        }],
        "targets": [{
            "$type": "database",
            "parameters": {
                "type": "postgre_s_q_l",
                "host": "localhost",
                "port": 5432,
                "database": "test",
                "user": "postgres",
                "password": "password",
                "schema": "public"
            },
            "schema_infos": [],
            "uid": "01999ff1-a268-7652-912e-6118d87da343",
            "type": "database"
        }]
    }
    
    result = service.process_new_format_request(real_request)
    
    print(f"[DATA] –°—Ç–∞—Ç—É—Å: {result.get('status')}")
    print(f"[MESSAGE] –°–æ–æ–±—â–µ–Ω–∏–µ: {result.get('message')}")
    print(f"[CONNECT] –ü–∞–π–ø–ª–∞–π–Ω–æ–≤: {result.get('pipeline_count', 0)}")
    print(f"[AI] AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {result.get('ai_recommendations', {}).get('storage_type', 'N/A')}")
    
    # –¢–µ—Å—Ç 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ Hadoop –ø—É—Ç–µ–π
    print("\\n=== –¢–µ—Å—Ç 2: Hadoop –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è ===")
    pipelines = result.get('pipelines', [])
    hadoop_functions = [p for p in pipelines if 'hadoop' in p.get('function_body', '').lower()]
    print(f"[HADOOP] –§—É–Ω–∫—Ü–∏–π —Å Hadoop: {len(hadoop_functions)}")
    
    for func in hadoop_functions:
        print(f"   ‚Ä¢ {func['function_name_ru']} (level {func['level']})")
    
    # –¢–µ—Å—Ç 3: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
    print("\\n=== –¢–µ—Å—Ç 3: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ ===")
    for pipeline in pipelines:
        print(f"Level {pipeline['level']}: {pipeline['function_name_ru']}")
        print(f"   From: {pipeline['from'][:8]}... ‚Üí To: {pipeline['to'][:8]}...")
    
    # –¢–µ—Å—Ç 4: Spark –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    print("\\n=== –¢–µ—Å—Ç 4: Spark –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===")
    spark_config = service.generate_spark_configuration(pipelines)
    print(f"[SPARK] Spark —Ñ—É–Ω–∫—Ü–∏–π: {len(spark_config['optimized_functions'])}")
    
    print("\\n[SUCCESS] –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´! –°–µ—Ä–≤–∏—Å –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å –Ω–æ–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º.")


if __name__ == "__main__":
    test_updated_service()

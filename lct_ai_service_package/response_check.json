{
    "request_uid":  "test-kafka-fix-001",
    "pipeline_uid":  "pipeline-test-001",
    "timestamp":  "2025-10-02T10:11:15.322477",
    "status":  "success",
    "artifacts":  {
                      "ddl_script":  "DDL Ð²ÑÑÑÐ¾ÐµÐ½ Ð² DAG ÑÑÐ½ÐºÑÐ¸Ð¸ (create_target_schema)",
                      "dag_content":  "# SPARK DISABLED: Ð¡ÑÐ°Ð½Ð´Ð°ÑÑÐ½Ð¾Ðµ Ð²ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ (Ð²ÑÐµ Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ¸ \u003c 500MB Ð¸ \u003c 1M ÑÑÑÐ¾Ðº)\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\n# ÐÐ¾Ð½ÑÐ¸Ð³ÑÑÐ°ÑÐ¸Ñ DAG\ndefault_args = {\n    \u0027owner\u0027: \u0027lct-ai-service\u0027,\n    \u0027depends_on_past\u0027: False,\n    \u0027start_date\u0027: datetime(2025, 10, 1),\n    \u0027email_on_failure\u0027: False,\n    \u0027email_on_retry\u0027: False,\n    \u0027retries\u0027: 1,\n    \u0027retry_delay\u0027: timedelta(minutes=5),\n}\n\n# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DAG\ndag = DAG(\n    \u0027hadoop_migration_pipeline\u0027,\n    default_args=default_args,\n    description=\u0027AI-Ð³ÐµÐ½ÐµÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Hadoop\u0027,\n    schedule_interval=None,  # Ð ÑÑÐ½Ð¾Ð¹ Ð·Ð°Ð¿ÑÑÐº\n    catchup=False,\n    tags=[\u0027hadoop\u0027, \u0027migration\u0027, \u0027ai-generated\u0027]\n)\n\n# ÐÐ°Ð´Ð°ÑÐ¸ Ð±ÑÐ´ÑÑ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸ ÑÐµÑÐµÐ· pipelines",
                      "migration_report":  "Ð¡Ð¾Ð·Ð´Ð°Ð½ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ñ 6 ÑÑÐ°Ð¿Ð°Ð¼Ð¸:\n1. Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐµÐ»ÐµÐ²Ð¾Ð¹ ÑÑ\u0085ÐµÐ¼Ñ (ÑÑÐ¾Ð²ÐµÐ½Ñ 0)\n   Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ°Ð±Ð»Ð¸Ñ Ð¸ ÑÑÑÑÐºÑÑÑÑ Ð² ÑÐµÐ»ÐµÐ²Ð¾Ð¹ Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085\n2. ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ° (ÑÑÐ¾Ð²ÐµÐ½Ñ 1)\n   Ð§ÑÐµÐ½Ð¸Ðµ Ð¸ Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Ð¸ÑÑ\u0085Ð¾Ð´Ð½Ð¾Ð³Ð¾ CSV ÑÐ°Ð¹Ð»Ð°\n3. Ð¢ÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ Ð¸ Ð¾ÑÐ¸ÑÑÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 (ÑÑÐ¾Ð²ÐµÐ½Ñ 2)\n   ÐÑÐµÐ¾Ð±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð¾ÑÐ¸ÑÑÐºÐ° Ð¸ Ð¿Ð¾Ð´Ð³Ð¾ÑÐ¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 Ðº Ð·Ð°Ð³ÑÑÐ·ÐºÐµ\n4. ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð² ÑÐµÐ»ÐµÐ²ÑÑ ÐÐ (ÑÑÐ¾Ð²ÐµÐ½Ñ 3)\n   ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐ°Ð½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð² ÑÐµÐ»ÐµÐ²ÑÑ Ð±Ð°Ð·Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085\n5. ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085 (ÑÑÐ¾Ð²ÐµÐ½Ñ 4)\n   ÐÑÐ¾Ð²ÐµÑÐºÐ° ÐºÐ¾ÑÑÐµÐºÑÐ½Ð¾ÑÑÐ¸ Ð¸ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085\n6. ÐÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¾ÑÑÐµÑÐ° Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ (ÑÑÐ¾Ð²ÐµÐ½Ñ 5)\n   Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¸Ð½Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¾ÑÑÐµÑÐ° Ð¾ ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐ°Ñ\u0085 Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085\n\nÐÐ±ÑÐ°Ñ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ:\n- ÐÑÑÐ¾ÑÐ½Ð¸ÐºÐ¸ Ð´Ð°Ð½Ð½ÑÑ\u0085: 1\n- Ð¦ÐµÐ»ÐµÐ²ÑÐµ ÑÐ¸ÑÑÐµÐ¼Ñ: 1\n- Spark ÑÐ°ÑÐ¿ÑÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ: ÐÑÐºÐ»ÑÑÐµÐ½Ð¾\n- Ð Ð°Ð·Ð¼ÐµÑ DAG: 854 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²",
                      "pipelines":  [
                                        {
                                            "name":  "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐµÐ»ÐµÐ²Ð¾Ð¹ ÑÑ\u0085ÐµÐ¼Ñ",
                                            "description":  "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ°Ð±Ð»Ð¸Ñ Ð¸ ÑÑÑÑÐºÑÑÑÑ Ð² ÑÐµÐ»ÐµÐ²Ð¾Ð¹ Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "level":  0,
                                            "id":  "4cf0c1f9-8d60-4e27-a6a4-aebffbb195fd",
                                            "from":  "source_0",
                                            "to":  "target_0",
                                            "function_name":  "create_target_schema",
                                            "function_name_ru":  "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÑ\u0085ÐµÐ¼Ñ Ð² ÑÐ°ÑÐ³ÐµÑÐµ",
                                            "function_body":  "\ndef create_target_schema(**context):\n    \"\"\"Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐµÐ»ÐµÐ²Ð¾Ð¹ ÑÑ\u0085ÐµÐ¼Ñ Ð² Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085\"\"\"\n    import psycopg2\n    from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n    \n    try:\n        # ÐÐ¾Ð´ÐºÐ»ÑÑÐµÐ½Ð¸Ðµ Ðº ÐÐ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÑ\u0085ÐµÐ¼Ñ\n        conn = psycopg2.connect(\n            host=\"localhost\",\n            port=5432,\n            database=\"test\",\n            user=\"postgres\",\n            password=\"password\"\n        )\n        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n        cursor = conn.cursor()\n        \n        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ°Ð±Ð»Ð¸ÑÑ Ð´Ð»Ñ Ð¼Ð¸Ð³ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085\n        create_table_sql = \"\"\"\n        CREATE SCHEMA IF NOT EXISTS public;\n        \n        DROP TABLE IF EXISTS public.migrated_data;\n        \n        CREATE TABLE public.migrated_data (\n            id SERIAL PRIMARY KEY,\n            data JSONB,\n            processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            processing_version VARCHAR(10),\n            data_source VARCHAR(50),\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        );\n        \n        -- ÐÐ½Ð´ÐµÐºÑÑ Ð´Ð»Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸\n        CREATE INDEX IF NOT EXISTS idx_migrated_data_processed_at \n        ON public.migrated_data(processed_at);\n        \n        CREATE INDEX IF NOT EXISTS idx_migrated_data_source \n        ON public.migrated_data(data_source);\n        \"\"\"\n        \n        cursor.execute(create_table_sql)\n        print(\"[SUCCESS] Ð¦ÐµÐ»ÐµÐ²Ð°Ñ ÑÑ\u0085ÐµÐ¼Ð° ÑÑÐ¿ÐµÑÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð°\")\n        \n        cursor.close()\n        conn.close()\n        \n        return \"schema_created\"\n        \n    except Exception as e:\n        print(f\"[ERROR] ÐÑÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÑ\u0085ÐµÐ¼Ñ: {e}\")\n        raise\n"
                                        },
                                        {
                                            "name":  "ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ°",
                                            "description":  "Ð§ÑÐµÐ½Ð¸Ðµ Ð¸ Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Ð¸ÑÑ\u0085Ð¾Ð´Ð½Ð¾Ð³Ð¾ CSV ÑÐ°Ð¹Ð»Ð°",
                                            "level":  1,
                                            "id":  "da3bf8b4-4dd8-423c-b728-f5aa7f840cee",
                                            "from":  "source_0",
                                            "to":  "4cf0c1f9-8d60-4e27-a6a4-aebffbb195fd",
                                            "function_name":  "extract_data",
                                            "function_name_ru":  "ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Hadoop",
                                            "function_body":  "\ndef extract_data(**context):\n    \"\"\"ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Hadoop CSV ÑÐ°Ð¹Ð»Ð°\"\"\"\n    from pyspark.sql import SparkSession\n    import pandas as pd\n    \n    try:\n        # ÐÐ½Ð¸ÑÐ¸Ð°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Spark ÑÐµÑÑÐ¸Ð¸ Ð´Ð»Ñ ÑÐ°Ð±Ð¾ÑÑ Ñ Hadoop\n        spark = SparkSession.builder \\\n            .appName(\"LCT_Data_Extract\") \\\n            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n            .getOrCreate()\n        \n        # Ð§ÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Hadoop\n        hadoop_path = \"\"\n        print(f\"ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Hadoop: {hadoop_path}\")\n        \n        # Ð§ÑÐµÐ½Ð¸Ðµ CSV Ñ Ð½Ð°ÑÑÑÐ¾Ð¹ÐºÐ°Ð¼Ð¸\n        df = spark.read \\\n            .option(\"header\", \"true\") \\\n            .option(\"delimiter\", \",\") \\\n            .option(\"inferSchema\", \"true\") \\\n            .option(\"multiline\", \"true\") \\\n            .option(\"escape\", \"\\\"\") \\\n            .csv(hadoop_path)\n        \n        # ÐÐ°Ð·Ð¾Ð²Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ\n        row_count = df.count()\n        print(f\"ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {row_count}\")\n        \n        if row_count == 0:\n            raise ValueError(\"Ð¤Ð°Ð¹Ð» Ð¿ÑÑÑ Ð¸Ð»Ð¸ Ð½Ðµ ÑÐ¾Ð´ÐµÑÐ¶Ð¸Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085\")\n        \n        # ÐÐ¾Ð½Ð²ÐµÑÑÐ°ÑÐ¸Ñ Ð² Pandas Ð´Ð»Ñ Ð´Ð°Ð»ÑÐ½ÐµÐ¹ÑÐµÐ¹ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÐ¸ (ÐµÑÐ»Ð¸ Ð½ÑÐ¶Ð½Ð¾)\n        pandas_df = df.toPandas()\n        \n        # Ð¡Ð¾Ñ\u0085ÑÐ°Ð½ÐµÐ½Ð¸Ðµ Ð² Ð²ÑÐµÐ¼ÐµÐ½Ð½Ð¾Ðµ Ñ\u0085ÑÐ°Ð½Ð¸Ð»Ð¸ÑÐµ Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑÑÑÐ¸Ñ\u0085 Ð·Ð°Ð´Ð°Ñ\n        temp_path = \"/tmp/extracted_data.parquet\"\n        df.write.mode(\"overwrite\").parquet(temp_path)\n        \n        # ÐÐµÑÐµÐ´Ð°ÑÐ° Ð¼ÐµÑÐ°Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð² XCom\n        context[\u0027task_instance\u0027].xcom_push(\n            key=\u0027extracted_data_path\u0027, \n            value=temp_path\n        )\n        context[\u0027task_instance\u0027].xcom_push(\n            key=\u0027row_count\u0027, \n            value=row_count\n        )\n        context[\u0027task_instance\u0027].xcom_push(\n            key=\u0027schema\u0027, \n            value=df.schema.json()\n        )\n        \n        print(\"[SUCCESS] ÐÐ°Ð½Ð½ÑÐµ ÑÑÐ¿ÐµÑÐ½Ð¾ Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ñ Ð¸Ð· Hadoop\")\n        return temp_path\n        \n    except Exception as e:\n        print(f\"[ERROR] ÐÑÐ¸Ð±ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085: {e}\")\n        raise\n    finally:\n        if \u0027spark\u0027 in locals():\n            spark.stop()\n",
                                            "spark_distributed":  false
                                        },
                                        {
                                            "name":  "Ð¢ÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ Ð¸ Ð¾ÑÐ¸ÑÑÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "description":  "ÐÑÐµÐ¾Ð±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð¾ÑÐ¸ÑÑÐºÐ° Ð¸ Ð¿Ð¾Ð´Ð³Ð¾ÑÐ¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 Ðº Ð·Ð°Ð³ÑÑÐ·ÐºÐµ",
                                            "level":  2,
                                            "id":  "6a60617b-7077-4f13-833d-6076e022d368",
                                            "from":  "da3bf8b4-4dd8-423c-b728-f5aa7f840cee",
                                            "to":  "da3bf8b4-4dd8-423c-b728-f5aa7f840cee",
                                            "function_name":  "transform_data",
                                            "function_name_ru":  "Ð¢ÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ Ð¸ Ð¾ÑÐ¸ÑÑÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "function_body":  "\ndef transform_data(**context):\n    \"\"\"Ð¢ÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ñ Ð¿Ð¾Ð´Ð´ÐµÑÐ¶ÐºÐ¾Ð¹ Spark\"\"\"\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import *\n    from pyspark.sql.types import *\n    \n    try:\n        # ÐÐ½Ð¸ÑÐ¸Ð°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Spark Ð´Ð»Ñ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸\n        spark = SparkSession.builder \\\n            .appName(\"LCT_Data_Transform\") \\\n            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n            .getOrCreate()\n        \n        # ÐÐ¾Ð»ÑÑÐµÐ½Ð¸Ðµ Ð¿ÑÑÐ¸ Ðº Ð´Ð°Ð½Ð½ÑÐ¼ Ð¸Ð· Ð¿ÑÐµÐ´ÑÐ´ÑÑÐµÐ¹ Ð·Ð°Ð´Ð°ÑÐ¸\n        extracted_path = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027extract_data\u0027, \n            key=\u0027extracted_data_path\u0027\n        )\n        \n        print(f\"Ð¢ÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð·: {extracted_path}\")\n        \n        # ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085\n        df = spark.read.parquet(extracted_path)\n        \n        # AI-Ð³ÐµÐ½ÐµÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐµ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸\n        print(\"ÐÑÐ¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ AI ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¹...\")\n        \n        # 1. ÐÑÐ¸ÑÑÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085\n        df_clean = df.dropna(how=\u0027all\u0027)  # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»Ð½Ð¾ÑÑÑÑ Ð¿ÑÑÑÑÑ\u0085 ÑÑÑÐ¾Ðº\n        \n        # 2. Ð¡ÑÐ°Ð½Ð´Ð°ÑÑÐ¸Ð·Ð°ÑÐ¸Ñ ÑÑÑÐ¾ÐºÐ¾Ð²ÑÑ\u0085 Ð¿Ð¾Ð»ÐµÐ¹\n        string_columns = [f.name for f in df_clean.schema.fields if f.dataType == StringType()]\n        for col_name in string_columns:\n            df_clean = df_clean.withColumn(\n                col_name, \n                trim(upper(col(col_name)))\n            )\n        \n        # 3. ÐÐ¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼ÐµÑÐ°Ð´Ð°Ð½Ð½ÑÑ\u0085\n        df_transformed = df_clean \\\n            .withColumn(\"processed_at\", current_timestamp()) \\\n            .withColumn(\"processing_version\", lit(\"1.0\")) \\\n            .withColumn(\"data_source\", lit(\"hadoop_pipeline\"))\n        \n        # 4. ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ Ð¿Ð¾ÑÐ»Ðµ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸\n        transformed_count = df_transformed.count()\n        original_count = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027extract_data\u0027, \n            key=\u0027row_count\u0027\n        )\n        \n        data_quality_ratio = transformed_count / original_count if original_count \u003e 0 else 0\n        print(f\"ÐÐ°ÑÐµÑÑÐ²Ð¾ Ð´Ð°Ð½Ð½ÑÑ\u0085: {data_quality_ratio:.2%} ({transformed_count}/{original_count})\")\n        \n        if data_quality_ratio \u003c 0.8:\n            print(\"[WARNING] ÐÑÐµÐ´ÑÐ¿ÑÐµÐ¶Ð´ÐµÐ½Ð¸Ðµ: Ð½Ð¸Ð·ÐºÐ¾Ðµ ÐºÐ°ÑÐµÑÑÐ²Ð¾ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¿Ð¾ÑÐ»Ðµ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸\")\n        \n        # Ð¡Ð¾Ñ\u0085ÑÐ°Ð½ÐµÐ½Ð¸Ðµ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085\n        transformed_path = \"/tmp/transformed_data.parquet\"\n        df_transformed.write.mode(\"overwrite\").parquet(transformed_path)\n        \n        # ÐÐµÑÐµÐ´Ð°ÑÐ° Ð¼ÐµÑÐ°Ð´Ð°Ð½Ð½ÑÑ\u0085\n        context[\u0027task_instance\u0027].xcom_push(key=\u0027transformed_data_path\u0027, value=transformed_path)\n        context[\u0027task_instance\u0027].xcom_push(key=\u0027transformed_count\u0027, value=transformed_count)\n        context[\u0027task_instance\u0027].xcom_push(key=\u0027data_quality_ratio\u0027, value=data_quality_ratio)\n        \n        print(\"[SUCCESS] Ð¢ÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð·Ð°Ð²ÐµÑÑÐµÐ½Ð°\")\n        return transformed_path\n        \n    except Exception as e:\n        print(f\"[ERROR] ÐÑÐ¸Ð±ÐºÐ° ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸: {e}\")\n        raise\n    finally:\n        if \u0027spark\u0027 in locals():\n            spark.stop()\n",
                                            "spark_distributed":  false
                                        },
                                        {
                                            "name":  "ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð² ÑÐµÐ»ÐµÐ²ÑÑ ÐÐ",
                                            "description":  "ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐ°Ð½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð² ÑÐµÐ»ÐµÐ²ÑÑ Ð±Ð°Ð·Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "level":  3,
                                            "id":  "6923a0a6-d3d0-42f0-875b-95174e825567",
                                            "from":  "6a60617b-7077-4f13-833d-6076e022d368",
                                            "to":  "target_0",
                                            "function_name":  "load_data",
                                            "function_name_ru":  "ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð² ÑÐµÐ»ÐµÐ²ÑÑ ÐÐ",
                                            "function_body":  "\ndef load_data(**context):\n    \"\"\"ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð² ÑÐµÐ»ÐµÐ²ÑÑ Ð±Ð°Ð·Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085\"\"\"\n    from pyspark.sql import SparkSession\n    import psycopg2\n    from sqlalchemy import create_engine\n    \n    try:\n        # ÐÐ°ÑÐ°Ð¼ÐµÑÑÑ Ð¿Ð¾Ð´ÐºÐ»ÑÑÐµÐ½Ð¸Ñ Ðº ÐÐ\n        db_config = {\n            \"host\": \"localhost\",\n            \"port\": 5432,\n            \"database\": \"test\",\n            \"user\": \"postgres\",\n            \"password\": \"password\",\n            \"schema\": \"public\"\n        }\n        \n        # ÐÐ¾Ð»ÑÑÐµÐ½Ð¸Ðµ Ð¿ÑÑÐ¸ Ðº ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¼ Ð´Ð°Ð½Ð½ÑÐ¼\n        transformed_path = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027transform_data\u0027, \n            key=\u0027transformed_data_path\u0027\n        )\n        \n        # ÐÐ½Ð¸ÑÐ¸Ð°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Spark Ñ Ð¿Ð¾Ð´Ð´ÐµÑÐ¶ÐºÐ¾Ð¹ PostgreSQL\n        spark = SparkSession.builder \\\n            .appName(\"LCT_Data_Load\") \\\n            .config(\"spark.jars\", \"/opt/airflow/jars/postgresql-42.7.0.jar\") \\\n            .getOrCreate()\n        \n        # ÐÐ°Ð³ÑÑÐ·ÐºÐ° ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085\n        df = spark.read.parquet(transformed_path)\n        \n        # ÐÐ¾Ð´ÐºÐ»ÑÑÐµÐ½Ð¸Ðµ Ðº PostgreSQL\n        jdbc_url = f\"jdbc:postgresql://{db_config[\u0027host\u0027]}:{db_config[\u0027port\u0027]}/{db_config[\u0027database\u0027]}\"\n        \n        # ÐÐ°Ð¿Ð¸ÑÑ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð² ÐÐ\n        df.write \\\n            .format(\"jdbc\") \\\n            .option(\"url\", jdbc_url) \\\n            .option(\"dbtable\", f\"{db_config[\u0027schema\u0027]}.migrated_data\") \\\n            .option(\"user\", db_config[\u0027user\u0027]) \\\n            .option(\"password\", db_config[\u0027password\u0027]) \\\n            .option(\"driver\", \"org.postgresql.Driver\") \\\n            .mode(\"overwrite\") \\\n            .save()\n        \n        loaded_count = df.count()\n        \n        # ÐÐµÑÐ°Ð´Ð°Ð½Ð½ÑÐµ Ð·Ð°Ð³ÑÑÐ·ÐºÐ¸\n        context[\u0027task_instance\u0027].xcom_push(key=\u0027loaded_count\u0027, value=loaded_count)\n        context[\u0027task_instance\u0027].xcom_push(key=\u0027target_table\u0027, value=f\"{db_config[\u0027schema\u0027]}.migrated_data\")\n        \n        print(f\"[SUCCESS] ÐÐ°Ð³ÑÑÐ¶ÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {loaded_count}\")\n        return loaded_count\n        \n    except Exception as e:\n        print(f\"[ERROR] ÐÑÐ¸Ð±ÐºÐ° Ð·Ð°Ð³ÑÑÐ·ÐºÐ¸: {e}\")\n        raise\n    finally:\n        if \u0027spark\u0027 in locals():\n            spark.stop()\n"
                                        },
                                        {
                                            "name":  "ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "description":  "ÐÑÐ¾Ð²ÐµÑÐºÐ° ÐºÐ¾ÑÑÐµÐºÑÐ½Ð¾ÑÑÐ¸ Ð¸ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "level":  4,
                                            "id":  "f9151599-54e5-42fa-960d-72edd967d23e",
                                            "from":  "6923a0a6-d3d0-42f0-875b-95174e825567",
                                            "to":  "6923a0a6-d3d0-42f0-875b-95174e825567",
                                            "function_name":  "validate_data",
                                            "function_name_ru":  "ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "function_body":  "\ndef validate_data(**context):\n    \"\"\"ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085\"\"\"\n    import psycopg2\n    \n    try:\n        # ÐÐ¾Ð´ÐºÐ»ÑÑÐµÐ½Ð¸Ðµ Ðº ÐÐ Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°ÑÐ¸Ð¸\n        conn = psycopg2.connect(\n            host=\"localhost\",\n            port=5432,\n            database=\"test\",\n            user=\"postgres\",\n            password=\"password\"\n        )\n        cursor = conn.cursor()\n        \n        # ÐÑÐ¾Ð²ÐµÑÐºÐ° ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²Ð° Ð·Ð°Ð¿Ð¸ÑÐµÐ¹\n        cursor.execute(\"SELECT COUNT(*) FROM public.migrated_data\")\n        count = cursor.fetchone()[0]\n        \n        # ÐÐ¾Ð»ÑÑÐµÐ½Ð¸Ðµ Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼Ð¾Ð³Ð¾ ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²Ð° Ð¸Ð· Ð¿ÑÐµÐ´ÑÐ´ÑÑÐ¸Ñ\u0085 Ð·Ð°Ð´Ð°Ñ\n        expected_count = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027load_data\u0027, \n            key=\u0027loaded_count\u0027\n        )\n        \n        print(f\"ÐÐ°Ð³ÑÑÐ¶ÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {count}, Ð¾Ð¶Ð¸Ð´Ð°Ð»Ð¾ÑÑ: {expected_count}\")\n        \n        if count != expected_count:\n            raise ValueError(f\"ÐÐµÑÐ¾Ð¾ÑÐ²ÐµÑÑÑÐ²Ð¸Ðµ ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²Ð° Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {count} != {expected_count}\")\n        \n        # ÐÑÐ¾Ð²ÐµÑÐºÐ° ÐºÐ°ÑÐµÑÑÐ²Ð° Ð´Ð°Ð½Ð½ÑÑ\u0085\n        cursor.execute(\"\"\"\n            SELECT \n                COUNT(*) as total,\n                COUNT(DISTINCT data) as unique_records,\n                COUNT(*) FILTER (WHERE processed_at IS NOT NULL) as with_timestamp\n            FROM public.migrated_data\n        \"\"\")\n        \n        total, unique, with_ts = cursor.fetchone()\n        quality_score = (unique / total) * 100 if total \u003e 0 else 0\n        \n        print(f\"ÐÐ°ÑÐµÑÑÐ²Ð¾ Ð´Ð°Ð½Ð½ÑÑ\u0085: {quality_score:.1f}% ÑÐ½Ð¸ÐºÐ°Ð»ÑÐ½ÑÑ\u0085 Ð·Ð°Ð¿Ð¸ÑÐµÐ¹\")\n        \n        context[\u0027task_instance\u0027].xcom_push(key=\u0027validation_passed\u0027, value=True)\n        context[\u0027task_instance\u0027].xcom_push(key=\u0027quality_score\u0027, value=quality_score)\n        \n        cursor.close()\n        conn.close()\n        \n        print(\"[SUCCESS] ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¿ÑÐ¾Ð¹Ð´ÐµÐ½Ð°\")\n        return True\n        \n    except Exception as e:\n        print(f\"[ERROR] ÐÑÐ¸Ð±ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°ÑÐ¸Ð¸: {e}\")\n        raise\n"
                                        },
                                        {
                                            "name":  "ÐÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¾ÑÑÐµÑÐ° Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸",
                                            "description":  "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¸Ð½Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¾ÑÑÐµÑÐ° Ð¾ ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐ°Ñ\u0085 Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                            "level":  5,
                                            "id":  "02be3ffa-08b0-48c9-87fe-40a98a4c78d2",
                                            "from":  "f9151599-54e5-42fa-960d-72edd967d23e",
                                            "to":  "pipeline-test-001",
                                            "function_name":  "generate_report",
                                            "function_name_ru":  "ÐÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¾ÑÑÐµÑÐ° Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸",
                                            "function_body":  "\ndef generate_report(**context):\n    \"\"\"ÐÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÑÐ¸Ð½Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¾ÑÑÐµÑÐ° Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸\"\"\"\n    import json\n    from datetime import datetime\n    \n    try:\n        # Ð¡Ð±Ð¾Ñ Ð¼ÐµÑÑÐ¸Ðº Ð¸Ð· Ð²ÑÐµÑ\u0085 Ð·Ð°Ð´Ð°Ñ\n        extracted_count = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027extract_data\u0027, key=\u0027row_count\u0027\n        )\n        transformed_count = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027transform_data\u0027, key=\u0027transformed_count\u0027\n        )\n        loaded_count = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027load_data\u0027, key=\u0027loaded_count\u0027\n        )\n        quality_score = context[\u0027task_instance\u0027].xcom_pull(\n            task_ids=\u0027validate_data\u0027, key=\u0027quality_score\u0027\n        )\n        \n        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾ÑÑÐµÑÐ°\n        report = {\n            \"migration_summary\": {\n                \"pipeline_id\": \"pipeline-test-001\",\n                \"completed_at\": datetime.now().isoformat(),\n                \"status\": \"SUCCESS\",\n                \"data_flow\": {\n                    \"extracted_from_hadoop\": extracted_count,\n                    \"transformed\": transformed_count,\n                    \"loaded_to_db\": loaded_count,\n                    \"quality_score\": f\"{quality_score:.1f}%\"\n                },\n                \"performance\": {\n                    \"data_loss_ratio\": f\"{((extracted_count - loaded_count) / extracted_count * 100):.1f}%\" if extracted_count \u003e 0 else \"0%\",\n                    \"transformation_efficiency\": f\"{(transformed_count / extracted_count * 100):.1f}%\" if extracted_count \u003e 0 else \"0%\"\n                }\n            }\n        }\n        \n        print(\"[DATA] ÐÑÑÐµÑ Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸:\")\n        print(json.dumps(report, ensure_ascii=False, indent=2))\n        \n        context[\u0027task_instance\u0027].xcom_push(key=\u0027final_report\u0027, value=report)\n        \n        print(\"[SUCCESS] ÐÐ¸Ð³ÑÐ°ÑÐ¸Ñ Ð·Ð°Ð²ÐµÑÑÐµÐ½Ð° ÑÑÐ¿ÐµÑÐ½Ð¾!\")\n        return report\n        \n    except Exception as e:\n        print(f\"[ERROR] ÐÑÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¾ÑÑÐµÑÐ°: {e}\")\n        raise\n"
                                        }
                                    ]
                  },
    "dag_content":  "# SPARK DISABLED: Ð¡ÑÐ°Ð½Ð´Ð°ÑÑÐ½Ð¾Ðµ Ð²ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ (Ð²ÑÐµ Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ¸ \u003c 500MB Ð¸ \u003c 1M ÑÑÑÐ¾Ðº)\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\n# ÐÐ¾Ð½ÑÐ¸Ð³ÑÑÐ°ÑÐ¸Ñ DAG\ndefault_args = {\n    \u0027owner\u0027: \u0027lct-ai-service\u0027,\n    \u0027depends_on_past\u0027: False,\n    \u0027start_date\u0027: datetime(2025, 10, 1),\n    \u0027email_on_failure\u0027: False,\n    \u0027email_on_retry\u0027: False,\n    \u0027retries\u0027: 1,\n    \u0027retry_delay\u0027: timedelta(minutes=5),\n}\n\n# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DAG\ndag = DAG(\n    \u0027hadoop_migration_pipeline\u0027,\n    default_args=default_args,\n    description=\u0027AI-Ð³ÐµÐ½ÐµÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Hadoop\u0027,\n    schedule_interval=None,  # Ð ÑÑÐ½Ð¾Ð¹ Ð·Ð°Ð¿ÑÑÐº\n    catchup=False,\n    tags=[\u0027hadoop\u0027, \u0027migration\u0027, \u0027ai-generated\u0027]\n)\n\n# ÐÐ°Ð´Ð°ÑÐ¸ Ð±ÑÐ´ÑÑ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸ ÑÐµÑÐµÐ· pipelines",
    "dag_config":  "# SPARK DISABLED: Ð¡ÑÐ°Ð½Ð´Ð°ÑÑÐ½Ð¾Ðµ Ð²ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ (Ð²ÑÐµ Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ¸ \u003c 500MB Ð¸ \u003c 1M ÑÑÑÐ¾Ðº)\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\n# ÐÐ¾Ð½ÑÐ¸Ð³ÑÑÐ°ÑÐ¸Ñ DAG\ndefault_args = {\n    \u0027owner\u0027: \u0027lct-ai-service\u0027,\n    \u0027depends_on_past\u0027: False,\n    \u0027start_date\u0027: datetime(2025, 10, 1),\n    \u0027email_on_failure\u0027: False,\n    \u0027email_on_retry\u0027: False,\n    \u0027retries\u0027: 1,\n    \u0027retry_delay\u0027: timedelta(minutes=5),\n}\n\n# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DAG\ndag = DAG(\n    \u0027hadoop_migration_pipeline\u0027,\n    default_args=default_args,\n    description=\u0027AI-Ð³ÐµÐ½ÐµÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Hadoop\u0027,\n    schedule_interval=None,  # Ð ÑÑÐ½Ð¾Ð¹ Ð·Ð°Ð¿ÑÑÐº\n    catchup=False,\n    tags=[\u0027hadoop\u0027, \u0027migration\u0027, \u0027ai-generated\u0027]\n)\n\n# ÐÐ°Ð´Ð°ÑÐ¸ Ð±ÑÐ´ÑÑ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸ ÑÐµÑÐµÐ· pipelines",
    "migration_report":  "Ð¡Ð¾Ð·Ð´Ð°Ð½ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ñ 6 ÑÑÐ°Ð¿Ð°Ð¼Ð¸:\n1. Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐµÐ»ÐµÐ²Ð¾Ð¹ ÑÑ\u0085ÐµÐ¼Ñ (ÑÑÐ¾Ð²ÐµÐ½Ñ 0)\n   Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ°Ð±Ð»Ð¸Ñ Ð¸ ÑÑÑÑÐºÑÑÑÑ Ð² ÑÐµÐ»ÐµÐ²Ð¾Ð¹ Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085\n2. ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ° (ÑÑÐ¾Ð²ÐµÐ½Ñ 1)\n   Ð§ÑÐµÐ½Ð¸Ðµ Ð¸ Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· Ð¸ÑÑ\u0085Ð¾Ð´Ð½Ð¾Ð³Ð¾ CSV ÑÐ°Ð¹Ð»Ð°\n3. Ð¢ÑÐ°Ð½ÑÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ Ð¸ Ð¾ÑÐ¸ÑÑÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 (ÑÑÐ¾Ð²ÐµÐ½Ñ 2)\n   ÐÑÐµÐ¾Ð±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð¾ÑÐ¸ÑÑÐºÐ° Ð¸ Ð¿Ð¾Ð´Ð³Ð¾ÑÐ¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 Ðº Ð·Ð°Ð³ÑÑÐ·ÐºÐµ\n4. ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð² ÑÐµÐ»ÐµÐ²ÑÑ ÐÐ (ÑÑÐ¾Ð²ÐµÐ½Ñ 3)\n   ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐ°Ð½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð² ÑÐµÐ»ÐµÐ²ÑÑ Ð±Ð°Ð·Ñ Ð´Ð°Ð½Ð½ÑÑ\u0085\n5. ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085 (ÑÑÐ¾Ð²ÐµÐ½Ñ 4)\n   ÐÑÐ¾Ð²ÐµÑÐºÐ° ÐºÐ¾ÑÑÐµÐºÑÐ½Ð¾ÑÑÐ¸ Ð¸ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ Ð·Ð°Ð³ÑÑÐ¶ÐµÐ½Ð½ÑÑ\u0085 Ð´Ð°Ð½Ð½ÑÑ\u0085\n6. ÐÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¾ÑÑÐµÑÐ° Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ (ÑÑÐ¾Ð²ÐµÐ½Ñ 5)\n   Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¸Ð½Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¾ÑÑÐµÑÐ° Ð¾ ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐ°Ñ\u0085 Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ\u0085\n\nÐÐ±ÑÐ°Ñ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ñ:\n- ÐÑÑÐ¾ÑÐ½Ð¸ÐºÐ¸ Ð´Ð°Ð½Ð½ÑÑ\u0085: 1\n- Ð¦ÐµÐ»ÐµÐ²ÑÐµ ÑÐ¸ÑÑÐµÐ¼Ñ: 1\n- Spark ÑÐ°ÑÐ¿ÑÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ: ÐÑÐºÐ»ÑÑÐµÐ½Ð¾\n- Ð Ð°Ð·Ð¼ÐµÑ DAG: 854 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²",
    "ai_recommendations":  {
                               "storage_type":  "PostgreSQL",
                               "confidence":  0.95,
                               "reasoning":  "ÐÐ½Ð°Ð»Ð¸Ð· Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ¾Ð² Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÐµÑ: 1) ÐÐ±ÑÐµÐ¼ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð½ÐµÐ±Ð¾Ð»ÑÑÐ¾Ð¹ (15.5 ÐÐ, 1000 ÑÑÑÐ¾Ðº), ÑÑÐ¾ Ð¸Ð´ÐµÐ°Ð»ÑÐ½Ð¾ Ð¿Ð¾Ð´Ñ\u0085Ð¾Ð´Ð¸Ñ Ð´Ð»Ñ PostgreSQL; 2) Ð¦ÐµÐ»ÐµÐ²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° ÑÐ¶Ðµ Ð¾Ð¿ÑÐµÐ´ÐµÐ»ÐµÐ½Ð° ÐºÐ°Ðº PostgreSQL, ÑÑÐ¾ Ð¸ÑÐºÐ»ÑÑÐ°ÐµÑ Ð½ÐµÐ¾Ð±Ñ\u0085Ð¾Ð´Ð¸Ð¼Ð¾ÑÑÑ Ð²ÑÐ±Ð¾ÑÐ° Ð´ÑÑÐ³Ð¾Ð¹ Ð¡Ð£ÐÐ; 3) Ð¡ÑÑÑÐºÑÑÑÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¿ÑÐ¾ÑÑÐ°Ñ - Ð¾Ð´Ð½Ð° ÑÐ°Ð±Ð»Ð¸ÑÐ° Ñ Ð´Ð²ÑÐ¼Ñ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°Ð¼Ð¸ (id, name), Ð±ÐµÐ· ÑÐ»Ð¾Ð¶Ð½ÑÑ\u0085 ÑÐ¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½ÑÑ\u0085; 4) Ð Ð±Ð¸Ð·Ð½ÐµÑ-ÐºÐ¾Ð½ÑÐµÐºÑÑÐµ ÑÐºÐ°Ð·Ð°Ð½Ð° Ð¼Ð¸Ð³ÑÐ°ÑÐ¸Ñ Ð¸Ð· Hadoop Ð² PostgreSQL, ÑÑÐ¾ Ð¿Ð¾Ð´ÑÐ²ÐµÑÐ¶Ð´Ð°ÐµÑ ÑÐµÐ»ÐµÑÐ¾Ð¾Ð±ÑÐ°Ð·Ð½Ð¾ÑÑÑ Ð²ÑÐ±Ð¾ÑÐ°; 5) PostgreSQL Ð¾Ð±ÐµÑÐ¿ÐµÑÐ¸Ñ Ð¾ÑÐ»Ð¸ÑÐ½ÑÑ Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÑ Ð´Ð»Ñ ÑÐ°ÐºÐ¾Ð³Ð¾ Ð¾Ð±ÑÐµÐ¼Ð° Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸ Ð¿Ð¾Ð´Ð´ÐµÑÐ¶Ð¸Ð²Ð°ÐµÑ Ð½ÐµÐ¾Ð±Ñ\u0085Ð¾Ð´Ð¸Ð¼ÑÐµ Ð¾Ð¿ÐµÑÐ°ÑÐ¸Ð¸ ETL.",
                               "transformation_steps":  [
                                                            {
                                                                "step":  1,
                                                                "action":  "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐµÐ»ÐµÐ²Ð¾Ð¹ ÑÐ°Ð±Ð»Ð¸ÑÑ Ð² PostgreSQL",
                                                                "details":  "CREATE TABLE test_customers (id INTEGER PRIMARY KEY, name VARCHAR NOT NULL)"
                                                            },
                                                            {
                                                                "step":  2,
                                                                "action":  "ÐÐ·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð¸Ð· CSV ÑÐ°Ð¹Ð»Ð°",
                                                                "details":  "Ð§ÑÐµÐ½Ð¸Ðµ /data/test_customers.csv Ñ Ð²Ð°Ð»Ð¸Ð´Ð°ÑÐ¸ÐµÐ¹ ÑÑÑÑÐºÑÑÑÑ"
                                                            },
                                                            {
                                                                "step":  3,
                                                                "action":  "ÐÑÐ¾Ð²ÐµÑÐºÐ° ÐºÐ°ÑÐµÑÑÐ²Ð° Ð´Ð°Ð½Ð½ÑÑ\u0085",
                                                                "details":  "ÐÐµÑÐ¸ÑÐ¸ÐºÐ°ÑÐ¸Ñ ÑÐ½Ð¸ÐºÐ°Ð»ÑÐ½Ð¾ÑÑÐ¸ id, Ð¿ÑÐ¾Ð²ÐµÑÐºÐ° Ð¾ÑÑÑÑÑÑÐ²Ð¸Ñ NULL Ð² Ð¾Ð±ÑÐ·Ð°ÑÐµÐ»ÑÐ½ÑÑ\u0085 Ð¿Ð¾Ð»ÑÑ\u0085"
                                                            },
                                                            {
                                                                "step":  4,
                                                                "action":  "ÐÐ°Ð³ÑÑÐ·ÐºÐ° Ð´Ð°Ð½Ð½ÑÑ\u0085 Ð² PostgreSQL",
                                                                "details":  "ÐÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ COPY ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ Ð¸Ð»Ð¸ pg_bulkload Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð¹ Ð·Ð°Ð³ÑÑÐ·ÐºÐ¸"
                                                            },
                                                            {
                                                                "step":  5,
                                                                "action":  "ÐÐ°Ð»Ð¸Ð´Ð°ÑÐ¸Ñ ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐ¾Ð²",
                                                                "details":  "Ð¡ÑÐ°Ð²Ð½ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²Ð° ÑÑÑÐ¾Ðº Ð¸ Ð¿ÑÐ¾Ð²ÐµÑÐºÐ° ÑÐµÐ»Ð¾ÑÑÐ½Ð¾ÑÑÐ¸ Ð´Ð°Ð½Ð½ÑÑ\u0085"
                                                            }
                                                        ],
                               "performance_estimate":  {
                                                            "extraction_time":  "1-2 ÑÐµÐºÑÐ½Ð´Ñ",
                                                            "transformation_time":  "2-3 ÑÐµÐºÑÐ½Ð´Ñ",
                                                            "loading_time":  "1-2 ÑÐµÐºÑÐ½Ð´Ñ",
                                                            "total_estimated_time":  "4-7 ÑÐµÐºÑÐ½Ð´",
                                                            "throughput":  "~200-250 ÑÑÑÐ¾Ðº/ÑÐµÐº"
                                                        },
                               "cost_estimate":  {
                                                     "development_cost":  "ÐÐ¸Ð·ÐºÐ¸Ð¹ (2-4 ÑÐ°ÑÐ° ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐºÐ¸)",
                                                     "infrastructure_cost":  "ÐÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑÐ½ÑÐ¹ (Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑÑÐµÑÑÐ²ÑÑÑÐµÐ³Ð¾ PostgreSQL)",
                                                     "maintenance_cost":  "ÐÐ¸Ð·ÐºÐ¸Ð¹",
                                                     "total_cost_category":  "ÐÑÐµÐ½Ñ Ð½Ð¸Ð·ÐºÐ¸Ð¹"
                                                 }
                           },
    "hadoop_integration":  true,
    "pipeline_count":  6,
    "spark_distribution_enabled":  false,
    "spark_distributed_functions":  0,
    "processing_time":  23.156149
}

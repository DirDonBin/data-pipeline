x-airflow-service: &airflow-service
  build: ./airflow
  environment: &airflow-common-env
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: hackaton_digital_transformation_leaders_25
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CELERY__BROKER_URL: redis://redis-airflow:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:airflow@postgres-airflow:5432/airflow
      AIRFLOW__HADOOP__CONN_ID: hadoop_default
      AIRFLOW__HADOOP__HOST: hadoop-namenode
      AIRFLOW__HADOOP__PORT: 8020
      AIRFLOW__HADOOP__LOGIN: admin
      AIRFLOW__SPARK__CONN_ID: spark_default
      AIRFLOW__SPARK__HOST: spark-master
      AIRFLOW__SPARK__PORT: 7077
      AIRFLOW__SPARK__TOTAL_EXECUTOR_CORES: 4
      AIRFLOW__SPARK__DRIVER_MEMORY: 2g
      AIRFLOW__SPARK__EXECUTOR_MEMORY: 2g
  volumes: &airflow-common-volumes
      - ./volumes/airflow/dags:/opt/airflow/dags
      - ./volumes/airflow/logs:/opt/airflow/logs
      - ./volumes/airflow/config:/opt/airflow/config
      - ./volumes/airflow/plugins:/opt/airflow/plugins
  depends_on: &airflow-common-depends-on
      redis-airflow:
        condition: "service_started"
      postgres-airflow:
        condition: "service_started"

services:
  postgres-airflow:
    image: postgres:latest
    container_name: postgres-airflow
    hostname: postgres-airflow
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  redis-airflow:
    image: redis:latest
    container_name: redis-airflow
    hostname: redis-airflow
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - ./volumes/redis:/data

  airflow-webserver:
    <<: *airflow-service
    container_name: airflow-webserver
    hostname: airflow-webserver
    ports:
      - "8090:8080"
    command: api-server
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-service
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    command: scheduler

  airflow-dag-processor:
    <<: *airflow-service
    container_name: airflow-dag-processor
    hostname: airflow-dag-processor
    command: dag-processor
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker-1:
    <<: *airflow-service
    container_name: airflow-worker-1
    hostname: airflow-worker-1
    command: celery worker
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker-2:
    <<: *airflow-service
    container_name: airflow-worker-2
    hostname: airflow-worker-2
    command: celery worker
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-service
    container_name: airflow-init
    hostname: airflow-init
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    volumes: *airflow-common-volumes
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
          export AIRFLOW_UID=$$(id -u)
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        echo
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow initialization completed!"
        echo "Password file will be used by webserver from the image"
    depends_on:
      <<: *airflow-common-depends-on
    user: "0:0"

  airflow-flower:
    <<: *airflow-service
    container_name: airflow-flower
    hostname: airflow-flower
    ports:
      - "5555:5555"
    command: celery flower
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
